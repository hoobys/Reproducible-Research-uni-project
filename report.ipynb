{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c1a4eac2b1b2bc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Recreation of XGBoost Model Study\n",
    "\n",
    "This is a comprehensive recreation of the influential study on the XGBoost model, originally presented in:\n",
    "\n",
    "**Li, H., Cao, Y., Li, S., Zhao, J., & Sun, Y. (2020). XGBoost model and its application to personal credit evaluation. IEEE Intelligent Systems, 35(3), 52-61.**\n",
    "\n",
    "The purpose of this research is to reproduce the findings of the original study, delving into the intricacies of the XGBoost model and exploring its robust application in the field of personal credit evaluation. This model has gained significant attention for its efficiency and performance in predictive modeling, particularly in financial contexts.\n",
    "\n",
    "The original study utilized a dataset from Lending Club, a popular peer-to-peer lending platform. Unfortunately, the direct access to this specific dataset is no longer available. However, thanks to the data-sharing community on Kaggle, we can still access a similar dataset. You can find the dataset we will be using [here](https://www.kaggle.com/datasets/wordsforthewise/lending-club/data).\n",
    "\n",
    "Through this notebook, we aim to:\n",
    "\n",
    "- Provide a detailed recreation of the study's methodology and analysis process.\n",
    "- Compare our results with those obtained in the original study to assess consistency and accuracy.\n",
    "- Highlight the challenges and considerations in replicating real-world studies.\n",
    "- Offer insights into the practical implications of the findings for credit evaluation.\n",
    "\n",
    "## Authors\n",
    "\n",
    "This project is a collaborative effort by a dedicated team of data science enthusiasts:\n",
    "\n",
    "- **Adam Janczyszyn**\n",
    "- **Hubert Wojewoda**\n",
    "- **Jakub Wujec**\n",
    "- **Jakub Å»mujdzin**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b2e7c1aaaa155",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "To begin the analysis, we need to load the dataset from Kaggle. Ensure you have the necessary libraries installed and the dataset downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14645d65e2238949",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T17:12:41.679574Z",
     "start_time": "2024-05-23T17:12:40.236955Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krzyr\\AppData\\Local\\Temp\\ipykernel_11228\\41789672.py:3: DtypeWarning: Columns (129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/credit_card_2018.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/credit_card_2018.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242d8ae919438be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once the dataset is loaded, we proceed to preprocess the data in accordance with the methodology outlined in the original study. This involves several critical steps.\n",
    "\n",
    "Firstly, we map the target variable to binary values (0 and 1). Following this, we implement a monthly sampling strategy where we select four times as many good clients as bad clients. This is essential for balancing the dataset and aligning it with the approach used in the study.\n",
    "\n",
    "To facilitate our analysis, we have prepared many functions, encapsulated in .py files located within the rr_project folder. These functions streamline various preprocessing tasks, ensuring consistency and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf35fd625c87cc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T17:12:42.351930Z",
     "start_time": "2024-05-23T17:12:41.680587Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data shape: (127702, 153). \n",
      "Source data target distribution:\n",
      "0    124626\n",
      "1      3076\n",
      "Name: target, dtype: int64\n",
      "Bads for month 1: 420. Goods for month 1: 1680.\n",
      "Bads for month 2: 323. Goods for month 2: 1292.\n",
      "Bads for month 3: 390. Goods for month 3: 1560.\n",
      "Bads for month 4: 394. Goods for month 4: 1576.\n",
      "Bads for month 5: 361. Goods for month 5: 1444.\n",
      "Bads for month 6: 310. Goods for month 6: 1240.\n",
      "Bads for month 7: 271. Goods for month 7: 1084.\n",
      "Bads for month 8: 213. Goods for month 8: 852.\n",
      "Bads for month 9: 136. Goods for month 9: 544.\n",
      "Bads for month 10: 133. Goods for month 10: 532.\n",
      "Bads for month 11: 80. Goods for month 11: 320.\n",
      "Bads for month 12: 45. Goods for month 12: 180.\n",
      "Undersampled data shape: (15380, 153). \n",
      "Undersampled data target distribution:\n",
      "0    12304\n",
      "1     3076\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from rr_project.data_wrangling import preprocess_data\n",
    "\n",
    "preprocessed_df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83affb90b1342c6a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Addressing Initial Discrepancies\n",
    "\n",
    "At this stage, we encounter some discrepancies compared to the original study:\n",
    "\n",
    "- While the number of rows remains consistent, our dataset contains 10 additional columns (153 vs. 143). These extra columns are primarily metadata (e.g., download URL) and will be excluded from the analysis.\n",
    "- There is a notable difference in the number of \"bad\" clients. The original authors classified \"Fully Paid\", \"Current\", and \"In Grace Period\" as good clients, categorizing all others as bad clients. However, the original study reported over 5,000 defaults, whereas our dataset has just over 3,000. Consequently, our overall sampled row count decreases from approximately 25,000 to about 15,000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febefaf24cb285a2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Selection for Preprocessing\n",
    "\n",
    "Next, we conduct feature selection as part of our preprocessing. The original authors manually excluded over 30 columns and removed all features with a NaN proportion exceeding 0.5. We will adhere to this methodology to maintain consistency with the study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db610dd193f011d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T17:12:42.380534Z",
     "start_time": "2024-05-23T17:12:42.352934Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with over 50% missing values = 37. Index(['member_id', 'desc', 'orig_projected_additional_accrued_interest',\n",
      "       'hardship_type', 'hardship_last_payment_amount',\n",
      "       'hardship_payoff_balance_amount', 'hardship_loan_status',\n",
      "       'hardship_dpd', 'hardship_length', 'hardship_amount', 'deferral_term',\n",
      "       'hardship_status', 'hardship_reason', 'settlement_percentage',\n",
      "       'settlement_amount', 'settlement_status', 'settlement_term',\n",
      "       'sec_app_mths_since_last_major_derog', 'verification_status_joint',\n",
      "       'sec_app_revol_util', 'sec_app_collections_12_mths_ex_med',\n",
      "       'sec_app_open_acc', 'annual_inc_joint', 'dti_joint',\n",
      "       'sec_app_chargeoff_within_12_mths', 'sec_app_num_rev_accts',\n",
      "       'sec_app_open_act_il', 'sec_app_fico_range_high', 'sec_app_mort_acc',\n",
      "       'sec_app_inq_last_6mths', 'sec_app_fico_range_low', 'revol_bal_joint',\n",
      "       'mths_since_last_record', 'mths_since_recent_bc_dlq',\n",
      "       'mths_since_last_major_derog', 'mths_since_recent_revol_delinq',\n",
      "       'mths_since_last_delinq'],\n",
      "      dtype='object')\n",
      "Feature selected data shape:  (15380, 84)\n"
     ]
    }
   ],
   "source": [
    "from rr_project.data_wrangling import preprocessing_feature_selection\n",
    "\n",
    "feature_selected_df = preprocessing_feature_selection(preprocessed_df)\n",
    "\n",
    "print(\"Feature selected data shape: \", feature_selected_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0afd82e696f32f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Label Encoding\n",
    "\n",
    "In the subsequent step, the authors mentioned converting text data to numeric data, though they did not specify the exact method. We opted to use `LabelEncoder` from `sklearn.preprocessing`, which efficiently encodes categorical labels to numeric values. We will also display the encodings for transparency. Null values in these columns will be filled with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d05fdba81cec507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T17:12:42.412635Z",
     "start_time": "2024-05-23T17:12:42.380534Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for column 'term': {' 36 months': 0, ' 60 months': 1}\n",
      "Encoding for column 'grade': {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6}\n",
      "Encoding for column 'emp_length': {'0': 0, '1 year': 1, '10+ years': 2, '2 years': 3, '3 years': 4, '4 years': 5, '5 years': 6, '6 years': 7, '7 years': 8, '8 years': 9, '9 years': 10, '< 1 year': 11}\n",
      "Encoding for column 'home_ownership': {'ANY': 0, 'MORTGAGE': 1, 'OWN': 2, 'RENT': 3}\n",
      "Encoding for column 'verification_status': {'Not Verified': 0, 'Source Verified': 1, 'Verified': 2}\n",
      "Encoding for column 'pymnt_plan': {'n': 0, 'y': 1}\n",
      "Encoding for column 'title': {'Credit card refinancing': 0}\n",
      "Encoding for column 'application_type': {'Individual': 0, 'Joint App': 1}\n",
      "Encoding for column 'hardship_flag': {'N': 0, 'Y': 1}\n",
      "Encoding for column 'disbursement_method': {'Cash': 0, 'DirectPay': 1}\n",
      "Encoding for column 'debt_settlement_flag': {'N': 0, 'Y': 1}\n",
      "Label encoded data shape:  (15380, 84)\n"
     ]
    }
   ],
   "source": [
    "from rr_project.data_wrangling import label_encode_all\n",
    "\n",
    "label_encoded_df = label_encode_all(feature_selected_df)\n",
    "\n",
    "print(\"Label encoded data shape: \", label_encoded_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed379e47989f884",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "To handle missing values, we fill null entries with the mean of the respective columns, ensuring our dataset remains intact for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8c870103adee71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T17:12:42.439856Z",
     "start_time": "2024-05-23T17:12:42.413641Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled nulls data shape:  (15380, 84)\n"
     ]
    }
   ],
   "source": [
    "from rr_project.data_wrangling import fill_nulls_with_mean\n",
    "\n",
    "wrangled_df = fill_nulls_with_mean(label_encoded_df)\n",
    "\n",
    "print(\"Filled nulls data shape: \", wrangled_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802637cda1699202",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Selection by Information Value (IV)\n",
    "\n",
    "Finally, we perform variable selection based on Information Value (IV), as described in the study. IV involves binning continuous variables and calculating the Weight of Evidence (WoE) for each bin. The IV is then determined as the sum of the differences between the WoE and the natural logarithm of the proportion of good clients to bad clients, serving as a measure of a variable's predictive power. The authors selected variables with an IV over 0.02. Lacking specific binning details from the study, we opt for decile binning for variables with more than 10 distinct values; otherwise, each distinct value forms a separate bin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64961dc32a7e8d60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T17:13:14.784042Z",
     "start_time": "2024-05-23T17:13:14.302367Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Variable        IV\n",
      "0               int_rate  0.533063\n",
      "0                  grade  0.492798\n",
      "0   debt_settlement_flag  0.143604\n",
      "0         fico_range_low  0.118618\n",
      "0        fico_range_high  0.118618\n",
      "..                   ...       ...\n",
      "0     num_tl_90g_dpd_24m  0.000000\n",
      "0                  title  0.000000\n",
      "0       num_tl_120dpd_2m  0.000000\n",
      "0            policy_code  0.000000\n",
      "0                  month  0.000000\n",
      "\n",
      "[83 rows x 2 columns]\n",
      "Variables with IV lower than threshold:  ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'annual_inc', 'title', 'dti', 'delinq_2yrs', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'collections_12_mths_ex_med', 'policy_code', 'application_type', 'acc_now_delinq', 'tot_coll_amt', 'open_act_il', 'total_bal_il', 'all_util', 'total_cu_tl', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mort_acc', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'total_bal_ex_mort', 'total_il_high_credit_limit', 'month']\n",
      "IV selected data shape:  (15380, 39)\n"
     ]
    }
   ],
   "source": [
    "from rr_project.data_wrangling import iv_selection\n",
    "\n",
    "df_iv_selection, iv_values = iv_selection(wrangled_df, \"target\")\n",
    "\n",
    "print(\"IV selected data shape: \", df_iv_selection.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd75906",
   "metadata": {},
   "source": [
    "### Addressing Collinearity\n",
    "After selecting variables based on IV, the next step is to address collinearity. Collinear variables can distort model performance and lead to overfitting. To ensure the robustness of our analysis, we remove highly collinear variables.\n",
    "\n",
    "- **Identify Collinear Variables**: Using the IV values, we identify variables that exhibit high collinearity.\n",
    "- **Remove Collinear Variables**: We systematically remove these variables to retain only those that contribute uniquely to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90312f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped grade due to high correlation with int_rate and lower IV.\n",
      "Dropped fico_range_high due to high correlation with fico_range_low and lower IV.\n",
      "Dropped open_il_24m due to high correlation with open_il_12m and lower IV.\n",
      "Dropped open_rv_12m due to high correlation with open_acc_6m and lower IV.\n",
      "Dropped open_rv_12m due to high correlation with open_rv_24m and lower IV.\n",
      "Dropped open_rv_12m due to high correlation with acc_open_past_24mths and lower IV.\n",
      "Dropped open_rv_24m due to high correlation with acc_open_past_24mths and lower IV.\n",
      "Dropped tot_cur_bal due to high correlation with avg_cur_bal and lower IV.\n",
      "Dropped total_rev_hi_lim due to high correlation with bc_open_to_buy and lower IV.\n",
      "Dropped mo_sin_rcnt_rev_tl_op due to high correlation with mo_sin_rcnt_tl and lower IV.\n",
      "Dropped mths_since_recent_bc due to high correlation with mo_sin_rcnt_rev_tl_op and lower IV.\n",
      "Dropped num_tl_op_past_12m due to high correlation with open_acc_6m and lower IV.\n",
      "Dropped open_rv_12m due to high correlation with num_tl_op_past_12m and lower IV.\n",
      "Dropped open_rv_24m due to high correlation with num_tl_op_past_12m and lower IV.\n",
      "Dropped num_tl_op_past_12m due to high correlation with acc_open_past_24mths and lower IV.\n",
      "Dropped tot_cur_bal due to high correlation with tot_hi_cred_lim and lower IV.\n",
      "Dropped avg_cur_bal due to high correlation with tot_hi_cred_lim and lower IV.\n",
      "Dropped total_rev_hi_lim due to high correlation with total_bc_limit and lower IV.\n",
      "Dropped bc_open_to_buy due to high correlation with total_bc_limit and lower IV.\n",
      "Dropped pymnt_plan due to high correlation with hardship_flag and lower IV.\n",
      "Filtered data shape:  (15380, 26)\n",
      "Dropped columns due to collinearity:  {'num_tl_op_past_12m', 'fico_range_high', 'tot_cur_bal', 'mo_sin_rcnt_rev_tl_op', 'open_il_24m', 'open_rv_12m', 'pymnt_plan', 'open_rv_24m', 'bc_open_to_buy', 'total_rev_hi_lim', 'avg_cur_bal', 'grade', 'mths_since_recent_bc'}\n"
     ]
    }
   ],
   "source": [
    "from rr_project.data_wrangling import remove_collinear_variables\n",
    "\n",
    "filtered_df, dropped_columns = remove_collinear_variables(df_iv_selection, iv_values)\n",
    "\n",
    "print(\"Filtered data shape: \", filtered_df.shape)\n",
    "print(\"Dropped columns due to collinearity: \", dropped_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ebd21",
   "metadata": {},
   "source": [
    "The output shows that several variables were dropped due to high correlation with others and lower Information Value (IV). These dropped variables included those highly correlated with key features like `int_rate`, `fico_range_low`, and `total_bc_limit`, among others. The filtered dataset now has 15,380 rows and 26 columns. Now we can be sure that only the most predictive and non-redundant features are retained for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
